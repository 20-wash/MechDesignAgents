{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarforever/AutoGen-Tutorials/blob/main/autogen_rag_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysXjb6M0gkhj"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_web_info.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39_S0tWcB-oh"
      },
      "source": [
        "# AutoGen Agents with Retrieval Augmented Generation\n",
        "\n",
        "**`AutoGen`** is a versatile framework that facilitates the creation of LLM applications by employing multiple agents capable of interacting with one another to tackle tasks. These AutoGen agents can be tailored to specific needs, engage in conversations, and seamlessly integrate human participation. They are adaptable to different operation modes that encompass the utilization of LLMs, human inputs, and various tools.\n",
        "\n",
        "**`RAG`** stands for `Retrieval Augmented Generation`, a natural language processing (NLP) technique that combines two essential components: **retrieval** and **generation**.\n",
        "\n",
        "The previous tutorial [AutoGen + LangChain = Super AI Agents](https://github.com/sugarforever/LangChain-Advanced/blob/main/Integrations/AutoGen/autogen_langchain_uniswap_ai_agent.ipynb) introduced how to build an AutoGen application that can execute tasks requiring specific documents knowledge. This is a typical RAG use case, aka. document based chatbot.\n",
        "\n",
        "The latest **AutoGen** version already supports RAG natively with the feature package `retrievechat`.\n",
        "\n",
        "In this tutorial, we are going to rebuild the same feature demonstrated in the previous tutorial. We will utilize `AutoGen` `retrievechat` feature.\n",
        "\n",
        "This tutorial is inspired by the [Blog - Retrieval-Augmented Generation (RAG) Applications with AutoGen](https://microsoft.github.io/autogen/blog/2023/10/18/RetrieveChat) of [Li Jiang](https://github.com/thinkall).\n",
        "\n",
        "Credits go to Li Jiang! üôå\n",
        "\n",
        "Let's roll! üö¥üèª‚Äç‚ôÄÔ∏è üö¥üèª üö¥üèª‚Äç‚ôÇÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtvjgnBZZjUL"
      },
      "source": [
        "## Use Case\n",
        "\n",
        "\n",
        "\n",
        "In this tutorial, I will create the retrieval augmented agents with the following document:\n",
        "\n",
        "[RETRIEVAL AUGMENTED GENERATION AND REPRESENTATIVE\n",
        "VECTOR SUMMARIZATION FOR LARGE UNSTRUCTURED\n",
        "TEXTUAL DATA IN MEDICAL EDUCATION](https://arxiv.org/pdf/2308.00479.pdf)\n",
        "\n",
        "You should be able to see the agents are able to perform retrieval augmented generation based on the document above and answer question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z73o7bmtb5LH"
      },
      "source": [
        "### Environment Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
          "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
          "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
          "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
        },
        "id": "1VRZnGGGgkhl"
      },
      "outputs": [],
      "source": [
        "%pip install pyautogen[retrievechat] langchain \"chromadb<0.4.15\" -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HZ7w_A3nXU8-"
      },
      "outputs": [],
      "source": [
        "import autogen\n",
        "\n",
        "config_list_gemini = [\n",
        "    {\n",
        "        \"model\": 'gemini-pro',\n",
        "        \"api_key\": 'AIzaSyBEx8PESd9f8ff1IqMSQ2usB-cLngPZLug',  # Replace with your API key variable\n",
        "        \"api_type\": \"google\",\n",
        "    }\n",
        "]\n",
        "\n",
        "gemini_config = {\n",
        "    \"seed\": 25,\n",
        "    \"temperature\": 0,\n",
        "    \"config_list\": config_list_gemini,\n",
        "    \"request_timeout\": 600,\n",
        "    \"retry_wait_time\": 120,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA48TH6Hc_3c"
      },
      "source": [
        "### Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCrCnRC7cdC-"
      },
      "source": [
        "#### 1. Configure Embedding Function\n",
        "\n",
        "We will use OpenAI embedding function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c636oZ2zHNpr"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(google_api_key=\"AIzaSyBEx8PESd9f8ff1IqMSQ2usB-cLngPZLug\",model=\"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxFsXiHVciOo"
      },
      "source": [
        "#### 2. Configure Text Splitter\n",
        "\n",
        "LangChain has done a great job in text splitting, so we will use its components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6eRvVjJITKfR"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "vectorstore_disk = Chroma(\n",
        "                        persist_directory=\"/home/niel77/MechanicalAgents/chroma_db\",       # Directory of db\n",
        "                        embedding_function=embeddings   # Embedding model\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7VPXVI_coX4"
      },
      "source": [
        "#### 3. Configure Vector Store\n",
        "\n",
        "By default, the AutoGen retrieval augmented agents use `chromadb` as vector store.\n",
        "\n",
        "Developers can configure preferred vector store by extending the class `RetrieveUserProxyAgent` and overriding function `retrieve_docs`.\n",
        "\n",
        "AutoGen also supports simple configuration items to customize Chromadb storage.\n",
        "\n",
        "In this demo, we will specify the collection name by `retreive_config` item `collection_name`. You should be able to see it in step 4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu7gjAv-c4uP"
      },
      "source": [
        "#### 4. Create Retrieval Augmented Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZsXuHf1fgkhl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_17038/1280325308.py:10: DeprecationWarning: The RetrieveAssistantAgent is deprecated. Please use the AssistantAgent instead.\n",
            "  assistant = RetrieveAssistantAgent(\n"
          ]
        }
      ],
      "source": [
        "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
        "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
        "\n",
        "llm_config = {\n",
        "    \"request_timeout\": 600,\n",
        "    \"config_list\": config_list_gemini,\n",
        "    \"temperature\": 0\n",
        "}\n",
        "\n",
        "assistant = RetrieveAssistantAgent(\n",
        "    name=\"assistant\",\n",
        "    system_message=\"You are a helpful assistant.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "rag_agent = RetrieveUserProxyAgent(\n",
        "    human_input_mode=\"NEVER\",\n",
        "    retrieve_config={\n",
        "        \"task\": \"qa\",\n",
        "        \"docs_path\": \"/home/niel77/MechanicalAgents/data/Examples_small.pdf\",\n",
        "        \"collection_name\": \"rag_collection\",\n",
        "        \"embedding_function\": embeddings,\n",
        "        \"custom_text_split_function\": text_splitter.split_documents,\n",
        "    },\n",
        "    code_execution_config= {\n",
        "        \"work_dir\": \"NewCAD1\",\n",
        "        \"use_docker\": False,\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37cRtpqLdLSZ"
      },
      "source": [
        "### It's time to start a chat with the RAG agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCdAqig3gkhn",
        "outputId": "d4463f2e-7dcc-4fa1-8552-b9dab3291afd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mRetrieveChatAgent\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to RetrieveChatAgent):\n",
            "\n",
            "As a helpful assistant, I am here to provide you with the best possible support and assistance. I can help you with a wide range of tasks, including:\n",
            "\n",
            "* Answering your questions\n",
            "* Providing information\n",
            "* Completing tasks\n",
            "* Solving problems\n",
            "* Making recommendations\n",
            "* And much more!\n",
            "\n",
            "I am constantly learning and improving, so I can provide you with the best possible experience. I am also available 24/7, so you can always get the help you need, when you need it.\n",
            "\n",
            "Here are some examples of how I can help you:\n",
            "\n",
            "* I can help you find information on the web.\n",
            "* I can help you write emails and other documents.\n",
            "* I can help you manage your schedule and appointments.\n",
            "* I can help you track your expenses and budget.\n",
            "* I can help you learn new skills.\n",
            "* I can help you troubleshoot problems.\n",
            "* I can help you make decisions.\n",
            "* I can help you relax and de-stress.\n",
            "\n",
            "I am here to help you with whatever you need, so please don't hesitate to ask. I am always happy to help!\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ChatResult(chat_id=None, chat_history=[{'content': '', 'role': 'assistant', 'name': 'RetrieveChatAgent'}, {'content': \"As a helpful assistant, I am here to provide you with the best possible support and assistance. I can help you with a wide range of tasks, including:\\n\\n* Answering your questions\\n* Providing information\\n* Completing tasks\\n* Solving problems\\n* Making recommendations\\n* And much more!\\n\\nI am constantly learning and improving, so I can provide you with the best possible experience. I am also available 24/7, so you can always get the help you need, when you need it.\\n\\nHere are some examples of how I can help you:\\n\\n* I can help you find information on the web.\\n* I can help you write emails and other documents.\\n* I can help you manage your schedule and appointments.\\n* I can help you track your expenses and budget.\\n* I can help you learn new skills.\\n* I can help you troubleshoot problems.\\n* I can help you make decisions.\\n* I can help you relax and de-stress.\\n\\nI am here to help you with whatever you need, so please don't hesitate to ask. I am always happy to help!\", 'role': 'user', 'name': 'assistant'}], summary=\"As a helpful assistant, I am here to provide you with the best possible support and assistance. I can help you with a wide range of tasks, including:\\n\\n* Answering your questions\\n* Providing information\\n* Completing tasks\\n* Solving problems\\n* Making recommendations\\n* And much more!\\n\\nI am constantly learning and improving, so I can provide you with the best possible experience. I am also available 24/7, so you can always get the help you need, when you need it.\\n\\nHere are some examples of how I can help you:\\n\\n* I can help you find information on the web.\\n* I can help you write emails and other documents.\\n* I can help you manage your schedule and appointments.\\n* I can help you track your expenses and budget.\\n* I can help you learn new skills.\\n* I can help you troubleshoot problems.\\n* I can help you make decisions.\\n* I can help you relax and de-stress.\\n\\nI am here to help you with whatever you need, so please don't hesitate to ask. I am always happy to help!\", cost={'usage_including_cached_inference': {'total_cost': 0.0003505, 'gemini-pro': {'cost': 0.0003505, 'prompt_tokens': 8, 'completion_tokens': 231, 'total_tokens': 239}}, 'usage_excluding_cached_inference': {'total_cost': 0.0003505, 'gemini-pro': {'cost': 0.0003505, 'prompt_tokens': 8, 'completion_tokens': 231, 'total_tokens': 239}}}, human_input=[''])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant.reset()\n",
        "rag_agent.initiate_chat(assistant, problem=\"What is the method to create hole in CadQuery?\", n_results=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
